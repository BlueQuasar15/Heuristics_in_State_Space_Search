# -*- coding: utf-8 -*-
"""AI_LAB2_A2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nh7VR94W79nDGakV33s_GtJnE4e_Ce5f
"""

from queue import Queue, LifoQueue
import heapq

class Node:
  action = None

  def __init__(self, state, path_cost=0, heuristic=0,  parent=None):
    self.state = state
    self.path_cost = path_cost
    self.heuristic = heuristic
    self.parent = parent

  def __lt__(self, other):
    return self.get_total_cost() < other.get_total_cost()

  def get_total_cost(self):
    return self.path_cost + self.heuristic

  def add_action(self):
    return

class Node_bfs:
  action = None

  def __init__(self, state, path_cost=0, parent=None):
    self.state = state
    self.path_cost = path_cost
    self.parent = parent

  def add_action(self):
    return

class Frontier:
  def __init__(self):
    self.elements = []

  def is_empty(self):
    return len(self.elements) == 0

  def add(self, node):
    heapq.heappush(self.elements, (node.get_total_cost(), node))

  def get(self):
    return heapq.heappop(self.elements)[1]

class Environment:
  initial_state = None
  goal_state = None

  def __init__(self, init_state, goal_state):
    self.initial_state = init_state
    self.goal_state = goal_state

  def transition(self, state):
    transitions = []

    for i in range(7):
      for j in range(7):
        if state[i][j] == 'O':
          if i - 2 >= 0 and state[i-1][j] == "O" and state[i-2][j] == '_':
            new_state = [row[:] for row in state]
            new_state[i][j] = '_'
            new_state[i-1][j] = '_'
            new_state[i-2][j] = 'O'
            transitions.append(new_state)
          if i + 2 < 7 and state[i+1][j] == 'O' and state[i+2][j] == '_':
            new_state = [row[:] for row in state]
            new_state[i][j] = '_'
            new_state[i+1][j] = '_'
            new_state[i+2][j] = 'O'
            transitions.append(new_state)
          if j - 2 >= 0 and state[i][j-1] == "O" and state[i][j-2] == '_':
            new_state = [row[:] for row in state]
            new_state[i][j] = '_'
            new_state[i][j-1] = '_'
            new_state[i][j-2] = 'O'
            transitions.append(new_state)
          if j + 2 < 7 and state[i][j+1] == 'O' and state[i][j+2] == '_':
            new_state = [row[:] for row in state]
            new_state[i][j] = '_'
            new_state[i][j+1] = '_'
            new_state[i][j+2] = 'O'
            transitions.append(new_state)

    return transitions


  def find_heuristic(self, state):
    return sum(row.count('O') for row in state)

  def is_goal(self, state):
    for i in range(7):
      for j in range(7):
        if state[i][j] != self.goal_state[i][j]: return False
    return True

class Agent:
  explored = set()
  frontier = Frontier()

  def search(self, env):
    curr_node = Node(env.initial_state, heuristic=env.find_heuristic(env.initial_state))
    self.frontier.add(curr_node)
    # print_states(curr_node)

    i = 1
    # total_nodes = 0;
    while not(self.frontier.is_empty()):
      # total_nodes += 1
      curr_node = self.frontier.get()

      if env.is_goal(curr_node.state): return self.backtrack(curr_node, i)
      # print('Node : ', i)
      # print_states(curr_node)
      self.explored.add(tuple(map(tuple, curr_node.state)))

      for succ in env.transition(curr_node.state):
        i += 1
        if tuple(map(tuple, succ)) in self.explored: continue

        new_node = Node(succ, path_cost=curr_node.path_cost + 1, heuristic=env.find_heuristic(succ), parent=curr_node)
        new_node.add_action()
        self.frontier.add(new_node)

        # print('Succ : ')
        # print_states(new_node)

      # i += 1
      # print(f"Total nodes {i}\n")
    return None

  def backtrack(self, node, total_nodes):
    path = [node]
    print(f"Total nodes searched Astar {total_nodes}\n")
    while not(node.parent is None):
      node = node.parent
      path.append(node)

    return path

class Frontier_bfs:
  def __init__(self):
    self.elements = Queue()

  def is_empty(self):
    return self.elements.empty()

  def add(self, node):
    self.elements.put(node)

  def get(self):
    return self.elements.get()

  def qsize(self):
      return self.elements.qsize()

# print('Optimal Solution : \n', 'Left '.ljust(5), 'Action'.center(20), 'Right'.rjust(5))
# for node in path:
#   print(f'{node.state[0]}, {node.state[1]}'.ljust(5), f'{node.action}'.center(20), f'{3 - node.state[0]}, {3 - node.state[1]}'.rjust(5))

class Environment_bfs:
  initial_state = None
  goal_state = None

  def __init__(self, init_state, goal_state):
    self.initial_state = init_state
    self.goal_state = goal_state

  def transition(self, state):
    transitions = []

    for i in range(7):
      for j in range(7):
        if state[i][j] == 'O':
          if i - 2 >= 0 and state[i-1][j] == "O" and state[i-2][j] == '_':
            new_state = [row[:] for row in state]
            new_state[i][j] = '_'
            new_state[i-1][j] = '_'
            new_state[i-2][j] = 'O'
            transitions.append(new_state)
          if i + 2 < 7 and state[i+1][j] == 'O' and state[i+2][j] == '_':
            new_state = [row[:] for row in state]
            new_state[i][j] = '_'
            new_state[i+1][j] = '_'
            new_state[i+2][j] = 'O'
            transitions.append(new_state)
          if j - 2 >= 0 and state[i][j-1] == "O" and state[i][j-2] == '_':
            new_state = [row[:] for row in state]
            new_state[i][j] = '_'
            new_state[i][j-1] = '_'
            new_state[i][j-2] = 'O'
            transitions.append(new_state)
          if j + 2 < 7 and state[i][j+1] == 'O' and state[i][j+2] == '_':
            new_state = [row[:] for row in state]
            new_state[i][j] = '_'
            new_state[i][j+1] = '_'
            new_state[i][j+2] = 'O'
            transitions.append(new_state)

    return transitions

  def is_goal(self, state):
    for i in range(7):
      for j in range(7):
        if state[i][j] != self.goal_state[i][j]: return False
    return True

class Agent_bfs:
  explored = set()
  frontier = Frontier_bfs()

  def search(self, env):
    # curr_node = Node(env.initial_state, heuristic=env.find_heuristic(env.initial_state))
    curr_node = Node(env.initial_state)
    self.frontier.add(curr_node)
    # print_states(curr_node)

    print("Here 1")
    i = 1
    # total_nodes = 0
    while not(self.frontier.is_empty()):
      i += 1
      # total_nodes += 1
      # print(f"Here {t}   {self.frontier.qsize()}")
      curr_node = self.frontier.get()

      if env.is_goal(curr_node.state): return self.backtrack(curr_node)
      # print('Node : ', i)
      # print_states(curr_node)
      self.explored.add(tuple(map(tuple, curr_node.state)))

      for succ in env.transition(curr_node.state):
        if tuple(map(tuple, succ)) in self.explored: continue

        new_node = Node(succ, path_cost=curr_node.path_cost + 1, parent=curr_node)
        new_node.add_action()
        self.frontier.add(new_node)

        # print('Succ : ')
        # print_states(new_node)

      # i += 1
    # print(f"Total nodes {i}\n")
    return None




  def backtrack(self, node):
    path = [node]

    while not(node.parent is None):
      node = node.parent
      path.append(node)


    print("Here 4")
    return path

class Frontier_dfs:
  def __init__(self):
    self.elements = LifoQueue()

  def is_empty(self):
    return self.elements.empty()

  def add(self, node):
    self.elements.put(node)

  def get(self):
    return self.elements.get()

class Agent_dfs:
  explored = set()
  frontier = Frontier_dfs()

  def search(self, env):
    # curr_node = Node(env.initial_state, heuristic=env.find_heuristic(env.initial_state))
    curr_node = Node(env.initial_state)
    self.frontier.add(curr_node)
    # print_states(curr_node)
    i = 1
    # total_nodes = 0
    while not(self.frontier.is_empty()):
      # total_nodes += 1
      # print(f"Here {t}")
      curr_node = self.frontier.get()
      if env.is_goal(curr_node.state): return self.backtrack(curr_node, i)
      # print('Node : ', i)
      # print_states(curr_node)
      self.explored.add(tuple(map(tuple, curr_node.state)))
      for succ in env.transition(curr_node.state):
        i += 1
        if tuple(map(tuple, succ)) in self.explored: continue
        new_node = Node(succ, path_cost=curr_node.path_cost + 1, parent=curr_node)
        new_node.add_action()
        self.frontier.add(new_node)
        # print('Succ : ')
        # print_states(new_node)
      # print(f"Total nodes {i}\n")
    return None

  def backtrack(self, node, total_nodes):
    path = [node]
    print(f"Total nodes searched dfs {total_nodes}\n")
    while not(node.parent is None):
      node = node.parent
      path.append(node)
    return path

class Node_Befs:
  action = None

  def __init__(self, state, path_cost=0, heuristic=0,  parent=None):
    self.state = state
    self.path_cost = path_cost
    self.heuristic = heuristic
    self.parent = parent

  def __lt__(self, other):
    return self.get_total_cost() < other.get_total_cost()

  def get_total_cost(self):
    return self.path_cost + self.heuristic

  def add_action(self):
    return

class Frontier_Befs:
  def __init__(self):
    self.elements = []

  def is_empty(self):
    return len(self.elements) == 0

  def add(self, node):
    heapq.heappush(self.elements, (node.get_total_cost(), node))

  def get(self):
    return heapq.heappop(self.elements)[1]


class Environment_Befs:
  initial_state = None
  goal_state = None

  def __init__(self, init_state, goal_state):
    self.initial_state = init_state
    self.goal_state = goal_state

  def transition(self, state):
    transitions = []

    for i in range(7):
      for j in range(7):
        if state[i][j] == 'O':
          if i - 2 >= 0 and state[i-1][j] == "O" and state[i-2][j] == '_':
            new_state = [row[:] for row in state]
            new_state[i][j] = '_'
            new_state[i-1][j] = '_'
            new_state[i-2][j] = 'O'
            transitions.append(new_state)
          if i + 2 < 7 and state[i+1][j] == 'O' and state[i+2][j] == '_':
            new_state = [row[:] for row in state]
            new_state[i][j] = '_'
            new_state[i+1][j] = '_'
            new_state[i+2][j] = 'O'
            transitions.append(new_state)
          if j - 2 >= 0 and state[i][j-1] == "O" and state[i][j-2] == '_':
            new_state = [row[:] for row in state]
            new_state[i][j] = '_'
            new_state[i][j-1] = '_'
            new_state[i][j-2] = 'O'
            transitions.append(new_state)
          if j + 2 < 7 and state[i][j+1] == 'O' and state[i][j+2] == '_':
            new_state = [row[:] for row in state]
            new_state[i][j] = '_'
            new_state[i][j+1] = '_'
            new_state[i][j+2] = 'O'
            transitions.append(new_state)

    return transitions


  def find_heuristic(self, state):
    # return (sum(row.count('O') for row in state)/sum(row.count('_') for row in state))
    # return sum(row.count('O') for row in state)
    rows, cols = 7, 7
    center_row, center_col = rows // 2, cols // 2

    total_distance = 0
    total_pegs = 0

    for row in range(rows):
        for col in range(cols):
            if state[row][col] == 'O':  # Check if the cell contains a peg
                total_distance += abs(row - center_row) + abs(col - center_col)
                total_pegs += 1

    if total_pegs == 0:
        return 0  # Avoid division by zero

    # average_distance_to_center = total_distance / total_pegs
    return total_distance
    # return average_distance_to_center



  def is_goal(self, state):
    for i in range(7):
      for j in range(7):
        if state[i][j] != self.goal_state[i][j]: return False
    return True


class Agent_Befs:
  explored = set()
  frontier = Frontier()

  def search(self, env):
    curr_node = Node_Befs(env.initial_state, heuristic=env.find_heuristic(env.initial_state))
    self.frontier.add(curr_node)
    # print_states(curr_node)

    i = 1
    # total_nodes = 0;
    while not(self.frontier.is_empty()):
      # total_nodes += 1
      curr_node = self.frontier.get()

      if env.is_goal(curr_node.state): return self.backtrack(curr_node, i)
      # print('Node : ', i)
      # print_states(curr_node)
      self.explored.add(tuple(map(tuple, curr_node.state)))

      for succ in env.transition(curr_node.state):
        i += 1
        if tuple(map(tuple, succ)) in self.explored: continue

        new_node = Node(succ, path_cost=curr_node.path_cost + 1, heuristic=env.find_heuristic(succ), parent=curr_node)
        new_node.add_action()
        self.frontier.add(new_node)

        # print('Succ : ')
        # print_states(new_node)

      # i += 1
      # print(f"Total nodes {i}\n")
    return None

  def backtrack(self, node, total_nodes):
    path = [node]
    print(f"Total nodes searched BeFS {total_nodes}\n")
    while not(node.parent is None):
      node = node.parent
      path.append(node)

    return path

def print_states(n):
  for i in range(7):
    print('|', end='')
    for j in range(7):
      if n.state[i][j] == 'X' or n.state[i][j] == '_':print(' |', end='')
      else: print(f'{n.state[i][j]}|', end='')
    print()
  print()

initial_state = [
  ['X', 'X', 'O', 'O', 'O', 'X', 'X'],
  ['X', 'X', 'O', 'O', 'O', 'X', 'X'],
  ['O', 'O', 'O', 'O', 'O', 'O', 'O'],
  ['O', 'O', 'O', '_', 'O', 'O', 'O'],
  ['O', 'O', 'O', 'O', 'O', 'O', 'O'],
  ['X', 'X', 'O', 'O', 'O', 'X', 'X'],
  ['X', 'X', 'O', 'O', 'O', 'X', 'X'],
]

goal_state = [
  ['X', 'X', '_', '_', '_', 'X', 'X'],
  ['X', 'X', '_', '_', '_', 'X', 'X'],
  ['_', '_', '_', '_', '_', '_', '_'],
  ['_', '_', '_', 'O', '_', '_', '_'],
  ['_', '_', '_', '_', '_', '_', '_'],
  ['X', 'X', '_', '_', '_', 'X', 'X'],
  ['X', 'X', '_', '_', '_', 'X', 'X'],
]
# print_states(Node(initial_state))

agent1 = Agent_dfs()
env1 = Environment(initial_state, goal_state)
path1 = agent1.search(env1)
if path1 is None: print('Not possible')

agent2 = Agent() # A*
env2 = Environment(initial_state, goal_state)
path2 = agent2.search(env2)
if path2 is None: print('Not possible')

agent3 = Agent_Befs()
env3 = Environment_Befs(initial_state, goal_state)
path3 = agent3.search(env3)
if path3 is None: print('Not possible')

path1.reverse()
path2.reverse()

for n in path1:
  print_states(n)

for n in path2:
  print_states(n)